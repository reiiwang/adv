{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras /\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helpers\n",
    "# from Adverse import lowProFool, deepfool\n",
    "from Metrics import *\n",
    "from Adverse import deepfool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting and confirm the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed = 98\n",
    "num = 120\n",
    "num_seed = 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'lpf'+str(test_seed)+'_'+str(num)+'_'+str(num_seed)+'.csv'\n",
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('path + total_data.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_data = copy.deepcopy(data)\n",
    "ori_y = ori_data['isFake']\n",
    "ori_X = ori_data.drop(['isFake'],axis=1)\n",
    "ori_X_train, ori_X_test, ori_y_train, ori_y_test=train_test_split(ori_X,ori_y,test_size=0.2,random_state=test_seed,stratify=ori_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s1 = set(ori_X_test.index)\n",
    "s1 = np.array(list(s1))\n",
    "s = s1[s1<200]\n",
    "location = []\n",
    "loc = list(s*8)\n",
    "for i in range(len(loc)):\n",
    "    for j in range(8):\n",
    "        location.append(loc[i]+j)\n",
    "len(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(set(location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ori_X_test)\n",
    "len(ori_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample target to attack and creat a filter --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(num_seed)\n",
    "fea = random.sample(location,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.array(set(fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_fea_loc = []\n",
    "row_index = []\n",
    "for i in range(len(fea)):\n",
    "    att_fea_loc.append([fea[i]//8, fea[i]%8])\n",
    "    row_index.append(fea[i]//8)\n",
    "row_index = set(row_index)\n",
    "len(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = np.zeros((200, 8))\n",
    "fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(att_fea_loc)):\n",
    "    fil[att_fea_loc[i][0],att_fea_loc[i][1]] = 1.0\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "data_X = copy.deepcopy(data)\n",
    "data_X = data_X.drop(['isFake'],axis=1)\n",
    "# data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = list(np.max(data_X))\n",
    "min_data = list(np.min(data_X))\n",
    "# print(max_data)\n",
    "# print(min_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "target = 'isFake'\n",
    "feature_names = list(data.columns)\n",
    "feature_names = feature_names[0:8]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(df):\n",
    "    low_bounds = df.min().values\n",
    "    up_bounds = df.max().values\n",
    "    \n",
    "    #removing target WARNING ASSUMES TARGET IS LAST\n",
    "    low_bounds = low_bounds[0:-1]\n",
    "    up_bounds = up_bounds[0:-1]\n",
    "    \n",
    "    return [low_bounds, up_bounds]\n",
    "\n",
    "# Compute the bounds for clipping\n",
    "bounds = get_bounds(df)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    df_return = df.copy()\n",
    "    \n",
    "    # Makes sure target does not need scaling\n",
    "    targets = np.unique(df[target].values)\n",
    "    #assert(len(targets == 2) and 0 in targets and 1 in targets)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X = df_return[feature_names]\n",
    "    scaler.fit(X)    \n",
    "    df_return[feature_names] = scaler.transform(X)\n",
    "    \n",
    "    lower_bounds = scaler.transform([bounds[0]])\n",
    "    upper_bounds = scaler.transform([bounds[1]])\n",
    "\n",
    "    return scaler, df_return, (lower_bounds[0], upper_bounds[0])\n",
    "\n",
    "# Normalize the data\n",
    "scaler, df, bounds = normalize(df, target, feature_names, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.loc[row_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(df, target, show_heatmap=False):\n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = df.corr()\n",
    "    cor_target = abs(cor[target])\n",
    "\n",
    "    weights = cor_target[:-1] #removing target WARNING ASSUMES TARGET IS LAST\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "\n",
    "    if show_heatmap:\n",
    "        heatmap(cor)\n",
    "            \n",
    "    return weights.values\n",
    "\n",
    "# Compute the weihts modelizing the expert's knowledge\n",
    "weights = get_weights(df, target)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_valid(df):\n",
    "    # Train test splits (1:4)\n",
    "    # Test valid split (1:4)\n",
    "    df_train, df_test = train_test_split(df, test_size = 0.2, shuffle=True, random_state=SEED,stratify=df['isFake'])\n",
    "    df_test, df_valid = train_test_split(df_test, test_size = 0.2, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "# Split df into train/test/valid\n",
    "df_train, df_test = split_train_test_valid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_train)+len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_train is fake : ',len(df_train[df_train['isFake']==1]))\n",
    "print('df_train is real : ',len(df_train[df_train['isFake']==0]))\n",
    "print('df_test is fake : ',len(df_test[df_test['isFake']==1]))\n",
    "print('df_test is real : ',len(df_test[df_test['isFake']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "nu_1 = np.array([1., 0., 1., 1., 0., 0., 0., 1.]) # 只改前四低相關的特徵(1,3,4,8)\n",
    "nu_2 = np.array([0., 1., 0., 0., 1., 1., 1., 0.]) # 只改前四高相關的特徵(2,5,6,7)\n",
    "\n",
    "nu_3 = np.array([1., 1., 1., 0., 0., 0., 0., 0.])\n",
    "\n",
    "nu_4 = np.array([1., 1., 0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'Dataset'     : 'twitter',\n",
    "         'MaxIters'     : 10000,\n",
    "         'Alpha'        : 0.001,\n",
    "         'Lambda'       : 8.5,\n",
    "         'TrainData'    : df_train,\n",
    "         'TestData'     : df_test,\n",
    "         'Scaler'       : scaler,\n",
    "         'FeatureNames' : feature_names,\n",
    "         'Target'       : target,\n",
    "         'Weights'      : weights,\n",
    "         'Bounds'       : bounds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            a3 = self.linear3(h6)\n",
    "            y = self.softmax(a3)\n",
    "            return y\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N, n_classes):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0)) - 1) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
    "            # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "            preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            current_correct += (preds.int() == labels.int()).sum() /n_classes\n",
    "\n",
    "\n",
    "        current_loss = current_loss / X.size(0)\n",
    "        current_correct = current_correct.double() / X.size(0)    \n",
    "\n",
    "        return preds, current_loss, current_correct.item()\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[feature_names].values)\n",
    "    y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = y_train.size(1)\n",
    "\n",
    "    epochs = 6000\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-4    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        preds, epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size, n_classes)     \n",
    "        if (epoch % 1000 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy on test set\n",
    "y_true = df_test[target]\n",
    "x_test = torch.FloatTensor(df_test[feature_names].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "print(\"Recall score on test data\", recall_score(y_true, y_pred))\n",
    "print(\"F1 score on test data\", f1_score(y_true, y_pred))\n",
    "print(\"confusion_matrix\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config['TestData'] = temp\n",
    "# config['TestData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['TestData'] = df[df['isFake']==1].sample(n=75, random_state = 6)\n",
    "# config['TestData']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LowProFool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_gradients(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.grad is not None:\n",
    "            x.grad.detach_()\n",
    "            x.grad.zero_()\n",
    "    elif isinstance(x, collections.abc.Iterable):\n",
    "        for elem in x:\n",
    "            zero_gradients(elem)\n",
    "            \n",
    "# Clipping function\n",
    "def clip(current, low_bound, up_bound):\n",
    "    assert(len(current) == len(up_bound) and len(low_bound) == len(up_bound))\n",
    "    low_bound = torch.FloatTensor(low_bound)\n",
    "    #print('low_bound',low_bound)\n",
    "    up_bound = torch.FloatTensor(up_bound)\n",
    "    clipped = torch.max(torch.min(current, up_bound), low_bound)\n",
    "    #print(current)\n",
    "    #print('clipped',clipped)\n",
    "    return clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowProFool(x, model, weights, bounds, maxiters, alpha, lambda_, fil):\n",
    "    \"\"\"\n",
    "    Generates an adversarial examples x' from an original sample x\n",
    "\n",
    "    :param x: tabular sample\n",
    "    :param model: neural network\n",
    "    :param weights: feature importance vector associated with the dataset at hand\n",
    "    :param bounds: bounds of the datasets with respect to each feature\n",
    "    :param maxiters: maximum number of iterations ran to generate the adversarial examples\n",
    "    :param alpha: scaling factor used to control the growth of the perturbation\n",
    "    :param lambda_: trade off factor between fooling the classifier and generating imperceptible adversarial example\n",
    "    :return: original label prediction, final label prediction, adversarial examples x', iteration at which the class changed\n",
    "    \"\"\"\n",
    "#     print(x)\n",
    "#     r = Variable(torch.FloatTensor(1e-4 * np.ones(x.numpy().shape)), requires_grad=True)\n",
    "    #nu_1 = np.array([1., 0., 1., 1., 0., 0., 0., 1.]) # 只改前四低相關的特徵(1,3,4,8)\n",
    "    #nu_2 = np.array([0., 1., 0., 0., 1., 1., 1., 0.]) # 只改前四高相關的特徵(2,5,6,7)\n",
    "#     nu_3 = np.array([1., 1., 1., 0., 0., 0., 0., 0.]) # 前四高相關的特徵(2,5,6,7)\n",
    "#     nu_4 = np.array([1., 1., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "    r = Variable(torch.FloatTensor(1e-4 * fil), requires_grad=True)\n",
    "#     print(r)\n",
    "    v = torch.FloatTensor(np.array(weights))\n",
    "    #print(v)\n",
    "    output = model.forward(x + r)\n",
    "    orig_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
    "    #print(orig_pred)\n",
    "    target_pred = np.abs(1 - orig_pred)\n",
    "    #print(target_pred)\n",
    "    \n",
    "    target = [0., 1.] if target_pred == 1 else [1., 0.]\n",
    "    target = Variable(torch.tensor(target, requires_grad=False)) \n",
    "    \n",
    "    lambda_ = torch.tensor([lambda_])\n",
    "    \n",
    "    bce = nn.BCELoss()\n",
    "    l1 = lambda v, r: torch.sum(torch.abs(v * r)) #L1 norm\n",
    "    l2 = lambda v, r: torch.sqrt(torch.sum(torch.mul(v * r,v * r))) #L2 norm\n",
    "    \n",
    "    best_norm_weighted = np.inf\n",
    "    best_pert_x = x\n",
    "    \n",
    "    loop_i, loop_change_class = 0, 0\n",
    "#     while loop_i < 3:\n",
    "    while loop_i < maxiters:\n",
    "        #print('r',r)    \n",
    "        zero_gradients(r)\n",
    "        \n",
    "        # Computing loss \n",
    "        loss_1 = bce(output, target)\n",
    "        loss_2 = l2(v, r)\n",
    "        loss = (loss_1 + lambda_ * loss_2)\n",
    "\n",
    "        # Get the gradient\n",
    "        loss.backward(retain_graph=True)\n",
    "        grad_r = r.grad.data.cpu().numpy().copy()\n",
    "        #print(grad_r)\n",
    "        \n",
    "        # Guide perturbation to the negative of the gradient\n",
    "        ri = - grad_r\n",
    "    \n",
    "        # limit huge step\n",
    "        ri *= alpha\n",
    "\n",
    "        # Adds new perturbation to total perturbation\n",
    "        #r = r.clone().detach().cpu().numpy() + ri\n",
    "        \n",
    "        temp = []\n",
    "        for i in range(len(r)):\n",
    "            if r[i].tolist() == 0:\n",
    "                temp.append(0.0)\n",
    "                #temp.append(r.clone().detach().cpu().numpy()[i])\n",
    "            else:\n",
    "                temp.append(r.clone().detach().cpu().numpy()[i] + ri[i])\n",
    "        r = np.array(temp)\n",
    "        #print('r1',r)\n",
    "        \n",
    "        \n",
    "        # For later computation\n",
    "        r_norm_weighted = np.sum(np.abs(r * weights))\n",
    "        \n",
    "        # Ready to feed the model\n",
    "        r = Variable(torch.FloatTensor(r), requires_grad=True) \n",
    "        #print('r2',r)\n",
    "        \n",
    "        # Compute adversarial example\n",
    "        #print('x',x)\n",
    "        #print('r',r)\n",
    "        xprime = x + r\n",
    "#         if loop_i == 19999:\n",
    "\n",
    "#         print('xprime',xprime)\n",
    "        \n",
    "        # Clip to stay in legitimate bounds\n",
    "        xprime = clip(xprime, bounds[0], bounds[1])\n",
    "#         if loop_i == 19999:\n",
    "#         print('xprime',xprime)\n",
    "        \n",
    "        \n",
    "#         \n",
    "        \n",
    "        # Classify adversarial example\n",
    "        output = model.forward(xprime)\n",
    "        output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
    "        \n",
    "        # Keep the best adverse at each iterations\n",
    "        if output_pred != orig_pred and r_norm_weighted < best_norm_weighted:\n",
    "            best_norm_weighted = r_norm_weighted\n",
    "            best_pert_x = xprime\n",
    "\n",
    "        if output_pred == orig_pred:\n",
    "            loop_change_class += 1\n",
    "            \n",
    "#            \n",
    "#         print('xprime',xprime)\n",
    "        \n",
    "        loop_i += 1 \n",
    "        \n",
    "        \n",
    "    # Clip at the end no matter what\n",
    "    #print(best_pert_x)\n",
    "    best_pert_x = clip(best_pert_x, bounds[0], bounds[1])\n",
    "    output = model.forward(best_pert_x)\n",
    "    #print('output',output)\n",
    "    output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
    "    #print('output_pred',output_pred)\n",
    "    \n",
    "#     print(best_pert_x)\n",
    "#     x_temp = []\n",
    "#     for f in range(10):\n",
    "#         if fil[f] == 0.0:\n",
    "#             x_temp.append(x[f])\n",
    "#             print(x[f])\n",
    "#             print(best_pert_x[f])\n",
    "#         else:\n",
    "#             x_temp.append(best_pert_x[f])\n",
    "#     best_pert_x = torch.FloatTensor(x_temp) \n",
    "\n",
    "    return orig_pred, output_pred, best_pert_x.clone().detach().cpu().numpy(), loop_change_class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single data to obverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = config['TestData'].to_numpy()[0][0:10]\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tensor = torch.FloatTensor(c)\n",
    "# x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['Bounds']\n",
    "# fil[0][3] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, config['Model'], config['Weights'], config['Bounds'], config['MaxIters'], config['Alpha'], config['Lambda'],fil[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowProFool(x_tensor, config['Model'], config['Weights'], config['Bounds'], config['MaxIters'], config['Alpha'], config['Lambda'],fil[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(data.iloc[0]))\n",
    "# print(fil[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data_np = np.asarray(max_data)\n",
    "# min_data_np = np.asarray(min_data)\n",
    "# v =  x_adv.tolist()*(max_data_np-min_data_np)+min_data_np\n",
    "# adv_x = np.asarray([round(i) for i in v])\n",
    "# print(adv_x)\n",
    "# ori_x = data.iloc[0].tolist()[:-1]\n",
    "# print(ori_x)\n",
    "\n",
    "# new_adv_x = []\n",
    "# for i in range(10):\n",
    "#     if fil[0][i] == 0:\n",
    "#         new_adv_x.append(ori_x[i])\n",
    "#     else:\n",
    "#         new_adv_x.append(adv_x[i])\n",
    "# print(new_adv_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['TestData'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowprofool attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    #nu = config['nu']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
    "            \n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        \n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
    "        #print(x_tensor)\n",
    "        #print(row.name)\n",
    "        #print(fil[row.name])\n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, bounds,\n",
    "                                                             maxiters, alpha, lambda_,fil[row.name])\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
    "                                                          bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "            \n",
    "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
    "        #results[i] = adv_pred\n",
    "        \n",
    "        x = pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)\n",
    "  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_sample = gen_adv(config, 'LowProFool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the adversarial sample and output the dataset with adv.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv_sample_np = adv_sample[feature_names].to_numpy()\n",
    "max_data_np = np.asarray(max_data)\n",
    "min_data_np = np.asarray(min_data)\n",
    "original_np = []\n",
    "for i in range(len(adv_sample_np)):\n",
    "    v = adv_sample_np[i]*(max_data_np-min_data_np)+min_data_np\n",
    "    v = np.asarray([round(i) for i in v])\n",
    "    original_np.append(v)\n",
    "original = pd.DataFrame(original_np)\n",
    "original_f = pd.DataFrame(original_np)\n",
    "original['orig_pred'] = list(adv_sample['orig_pred'])\n",
    "original['adv_pred'] = list(adv_sample['adv_pred'])\n",
    "original.index = adv_sample.index\n",
    "original_f.index = adv_sample.index\n",
    "original_f.columns = feature_names\n",
    "original_f['isFake'] = [1 for i in range(len(row_index))]\n",
    "original_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_2 = copy.deepcopy(original)\n",
    "original_2 = original_2.drop('orig_pred',axis=1)\n",
    "original_2 = original_2.drop('adv_pred',axis=1)\n",
    "original_2.columns = feature_names\n",
    "original_2['isFake'] = [1 for i in range(len(row_index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_1 = data.iloc[list(set(list(data.index))-set(list(original_2.index)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = pd.concat((original_2,com_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = export.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
